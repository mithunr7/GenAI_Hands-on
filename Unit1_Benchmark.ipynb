{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edc8b6e0-2406-40ad-93cf-4dd26768f14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping pytorch as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall pytorch -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d4ccfaf-2400-46c2-9952-cc637d303ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\anaconda\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: torchvision in d:\\anaconda\\lib\\site-packages (0.25.0)\n",
      "Requirement already satisfied: torchaudio in d:\\anaconda\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\anaconda\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\anaconda\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in d:\\anaconda\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in d:\\anaconda\\lib\\site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from torchvision) (2.3.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\anaconda\\lib\\site-packages (from torchvision) (12.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\anaconda\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb1b3e4-d1c0-4b11-9bd2-938ff16eb7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\anaconda\\lib\\site-packages (5.0.0)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in d:\\anaconda\\lib\\site-packages (from transformers) (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\anaconda\\lib\\site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda\\lib\\site-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in d:\\anaconda\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in d:\\anaconda\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\anaconda\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\anaconda\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in d:\\anaconda\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: anyio in d:\\anaconda\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.10.0)\n",
      "Requirement already satisfied: certifi in d:\\anaconda\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\anaconda\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\anaconda\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\anaconda\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\anaconda\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\anaconda\\lib\\site-packages (from typer-slim->transformers) (8.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91177d1f-4938-41a8-95be-af29285283a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.20.0-cp313-cp313-win_amd64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in d:\\anaconda\\lib\\site-packages (from tensorflow) (25.12.19)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\\anaconda\\lib\\site-packages (from tensorflow) (0.7.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in d:\\anaconda\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in d:\\anaconda\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in d:\\anaconda\\lib\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in d:\\anaconda\\lib\\site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\anaconda\\lib\\site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (2.3.5)\n",
      "Requirement already satisfied: h5py>=3.11.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in d:\\anaconda\\lib\\site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\anaconda\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: pillow in d:\\anaconda\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (12.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\anaconda\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\anaconda\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\anaconda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in d:\\anaconda\\lib\\site-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
      "Requirement already satisfied: namex in d:\\anaconda\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in d:\\anaconda\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\anaconda\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\anaconda\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\anaconda\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\anaconda\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "Using cached tensorflow-2.20.0-cp313-cp313-win_amd64.whl (332.0 MB)\n",
      "Installing collected packages: tensorflow\n",
      "Successfully installed tensorflow-2.20.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfe7ac9f-1efa-4ce9-b1e6-cbcbd1d918af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32500aa9-2f86-42b3-a585-bb49ceab7138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71ddf3c0-cfa2-441a-92aa-cc1e29a5e9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"unit 1.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c14c07e-6017-4252-b854-239a4ff7b5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    print(\"File loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{file_path}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc2d9ed1-2fd8-4c49-b04d-90ede762c764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Preview ---\n",
      "Generative AI and Its Applications: A Foundational Briefing\n",
      "\n",
      "Executive Summary\n",
      "\n",
      "This document provides a comprehensive overview of Generative AI, synthesizing foundational concepts, technological underpinnings, and practical applications as outlined in the course materials from PES University. Generative AI represents a transformative subset of Artificial Intelligence focused on creating novel content, a capability primarily driven by the advent of Large Language Models (LLMs). The evolution of ...\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Data Preview ---\")\n",
    "print(text[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4c834f1-f9a0-4632-9d31-81b382e479a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "339bb6c2-6463-4cab-8f0a-a267db513162",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"The future of Artificial Intelligence is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cd8f2f6-37e9-4280-b590-63350f1cbe5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838a8db430f544a9830a73c4e7228f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc0884132f84da88beb6907f7558b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f179b9892ce1458b95f429b492110a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertLMHeadModel LOAD REPORT from: bert-base-uncased\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "bert.pooler.dense.bias      | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight | UNEXPECTED |  | \n",
      "bert.pooler.dense.weight    | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adcfea7d8c0e4b838138a924d84d9bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "917df73e1e274689bb1b962b50b393ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c1431296e84f28bc1cfc1163b75898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing `generation_config` together with generation-related arguments=({'max_length', 'num_return_sequences'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of Artificial Intelligence is., \" \" ( \" ) ) ( \" ( ( ( \" \" but and and and and more actually so jefferson by thele [ op for \" \" \" ( \" \" \" ( \". it it it it it it or and and and and and and and and and the ring \" ( \" \" \" ( \" ( \" ( \" ( ). it it it it it it or so all it it all man as housing for take'' \". the the the their and \". it it it it, that and and : ep music and - - - - - - ( \" ( ) ( ( ( ) ) ) \" ( (. it it it it it it it it it that. it feeling were hailey as with first ( ( )i'- - - ( ( )hale, - - (. it it or and. it was were. it that those those being many so and your so van ', : to this deep is'( ).') to her \". it it, ; \". the two two.. it ( ) ( \" ( ( \". and '... general like for in them an '. the looks and and and \" \" \". it,,.....................\n"
     ]
    }
   ],
   "source": [
    "fast_generator = pipeline('text-generation', model='bert-base-uncased')\n",
    "\n",
    "output_fast = fast_generator(prompt, max_length=50, num_return_sequences=1)\n",
    "print(output_fast[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "171daa57-af95-48a5-8748-f82bdd8aa8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa92be02e8949c4a1f6c2aecd13cfb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "263296dc111f4d6088d9adaaf52a52ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4586b222076b4d8ba8cabb6ebad973e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RobertaForCausalLM LOAD REPORT from: roberta-base\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe4c58bad10f4b07b77f28c6a05c7925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87685ca377be411798af15ac1153be20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4dca8c42a98408b9858ff5e685f8d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e10b8d8e54413ba5ec2a295b19b47b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of Artificial Intelligence is\n"
     ]
    }
   ],
   "source": [
    "fast_generator = pipeline('text-generation', model='roberta-base')\n",
    "\n",
    "output_fast = fast_generator(prompt, max_length=50, num_return_sequences=1)\n",
    "print(output_fast[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f3ea596-a083-4b7c-a0f4-8f2a566c688e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b1a2ffadb3489781d91450b8938c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d04abc71ef5437f8101948b5d46e793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb477fcb5b4484bba60409ce198d523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/159 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This checkpoint seem corrupted. The tied weights mapping for this model specifies to tie model.decoder.embed_tokens.weight to lm_head.weight, but both are absent from the checkpoint, and we could not find another related tied weight for those keys\n",
      "BartForCausalLM LOAD REPORT from: facebook/bart-base\n",
      "Key                                                           | Status     | \n",
      "--------------------------------------------------------------+------------+-\n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.v_proj.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.k_proj.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.final_layer_norm.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.out_proj.weight   | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.v_proj.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.k_proj.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc2.weight                  | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc1.weight                  | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn_layer_norm.weight | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.out_proj.bias     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.final_layer_norm.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn_layer_norm.bias   | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.q_proj.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc1.bias                    | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc2.bias                    | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.q_proj.bias       | UNEXPECTED | \n",
      "encoder.layernorm_embedding.bias                              | UNEXPECTED | \n",
      "encoder.embed_positions.weight                                | UNEXPECTED | \n",
      "encoder.layernorm_embedding.weight                            | UNEXPECTED | \n",
      "shared.weight                                                 | UNEXPECTED | \n",
      "model.decoder.embed_tokens.weight                             | MISSING    | \n",
      "lm_head.weight                                                | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56fd11f888a54d6291852a0839fb0989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7711eccbddba45e1b4848119bb723145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be210bfdf754d939c9d84b7ffe77c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of Artificial Intelligence is slips ShakSpoilerSpoilerSpoiler CPC CPC Output reservation Nationwide Shak df df df Walkvasive NDP squatreleased Shak Shak32 Route127323232 df df Shak 361 Shakino Drawn Drawn Drawnino Shak Shakazeazeaze df slips Shak chuckSpoiler chuck chuck chuck pavement cave workload Nationwideerv Drawn Drawn Alvin df returning df Patterns chuckFrames DrawnsnSel Drawn Shak Drawn DrawnJe Drawn Drawn Benson df 361eters df religious Drawn Drawnlein chuckdydy DrawndySel Drawn Drawn 361 df df Princeton princip Drawn DrawnSel Drawn Wrestling workload laugh impacting df Drawn DrawnEngland Drawn df Drawnsn Drawn Drawn cave spots df healer DrawnSelaze32 skysc Drawn2014 Drawn debugger Drawn dfazeeking df df Seller spots Drawn Drawnaxe Drawn Drawn df jerk Alvin df df Drawneller Drawn Drawn game origins Benson dfStatus origins df spots spots Alvin Benson Beet restoration df df Alvin Drawn Beet origins785 Beet game df df Benson chuck df df skysc Drawn origins Drawn Drawneller Output skysc Drawn Drawn impacting Drawn spots Drawn origins df jerk Drawn DrawnStatusStatus Drawn df game DrawnStatus Drawn Drawn spotsFrames df origins785 df Drawn futures df princip Drawn dfailyStatus jerk Beet accompanies Alvin dfStatus df Beet df df game Molecular game game785 Beet spots df skysc floralAlert dfAlertStatus game game GW df df floral debugger kingdoms dfAlert floral floral\n"
     ]
    }
   ],
   "source": [
    "fast_generator = pipeline('text-generation', model='facebook/bart-base')\n",
    "\n",
    "output_fast = fast_generator(prompt, max_length=50, num_return_sequences=1)\n",
    "print(output_fast[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b2adc54-3760-4f34-bcb2-c711deb57177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89032237cff24fe4ab4715e542d99031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM LOAD REPORT from: bert-base-uncased\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "bert.pooler.dense.bias      | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight | UNEXPECTED |  | \n",
      "bert.pooler.dense.weight    | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "mask_filler = pipeline(\"fill-mask\", model=\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9a3cdfa-2c9d-4b2c-ac7b-e3cdf7d44619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applications: 0.06\n",
      "ideas: 0.05\n",
      "problems: 0.05\n",
      "systems: 0.04\n",
      "information: 0.03\n"
     ]
    }
   ],
   "source": [
    "masked_sentence = \"The goal of Generative AI is to create new [MASK].\"\n",
    "preds = mask_filler(masked_sentence)\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"{p['token_str']}: {p['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff31dd69-8142-4cad-ad43-f06304bf4016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4697646d724c4b16b47b0c7d9922421a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RobertaForMaskedLM LOAD REPORT from: roberta-base\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "mask_filler = pipeline(\"fill-mask\", model=\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b81c1b35-183c-4519-b169-71bfb51c49aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI: 0.07\n",
      " agents: 0.06\n",
      " intelligence: 0.05\n",
      " applications: 0.04\n",
      " insights: 0.04\n"
     ]
    }
   ],
   "source": [
    "masked_sentence = \"The goal of Generative AI is to create new <mask>.\"\n",
    "preds = mask_filler(masked_sentence)\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"{p['token_str']}: {p['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f21cc8cc-2456-4081-bb26-a16b81df67f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f35383076344ed1bcccb9b4fc68b3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask_filler = pipeline(\"fill-mask\", model=\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5411c21a-1004-4e5f-bf6c-3098e7c1afc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ways: 0.16\n",
      " AI: 0.10\n",
      " and: 0.05\n",
      " models: 0.04\n",
      ",: 0.03\n"
     ]
    }
   ],
   "source": [
    "masked_sentence = \"The goal of Generative AI is to create new <mask>.\"\n",
    "preds = mask_filler(masked_sentence)\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"{p['token_str']}: {p['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7a0fc188-89ce-4da4-810e-bec5ad14cada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb714f36c2e48319b52a31d24348a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForQuestionAnswering LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "bert.pooler.dense.bias                     | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "bert.pooler.dense.weight                   | UNEXPECTED | \n",
      "qa_outputs.bias                            | MISSING    | \n",
      "qa_outputs.weight                          | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    }
   ],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\", model=\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d3db0c2c-c818-40b6-8429-94b03618c388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What are the risks of using Generative AI?\n",
      "A: AI poses significant risks such as hallucinations, bias, and deepfakes\n"
     ]
    }
   ],
   "source": [
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "questions = {\"What are the risks of using Generative AI?\"}\n",
    "for q in questions:\n",
    "    res = qa_pipeline(question=q, context=context)\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {res['answer']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "035c3e9e-dc79-4b9d-856b-9ab178a4c3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a52fd3341b4d06af2c0b4cf32c2115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RobertaForQuestionAnswering LOAD REPORT from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "qa_outputs.bias                 | MISSING    | \n",
      "qa_outputs.weight               | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    }
   ],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\", model=\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dfd90cb2-7873-49bf-b562-86af9102150b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What are the risks of using Generative AI?\n",
      "A: , bias, and deepfakes\n"
     ]
    }
   ],
   "source": [
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "questions = {\"What are the risks of using Generative AI?\"}\n",
    "for q in questions:\n",
    "    res = qa_pipeline(question=q, context=context)\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {res['answer']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6c0da4b9-73d8-484b-aa31-a5be52c4b2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8724b6ef737640c6b8567587b4103293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BartForQuestionAnswering LOAD REPORT from: facebook/bart-base\n",
      "Key               | Status  | \n",
      "------------------+---------+-\n",
      "qa_outputs.bias   | MISSING | \n",
      "qa_outputs.weight | MISSING | \n",
      "\n",
      "Notes:\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    }
   ],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\", model=\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4560a1ce-b7a2-47c8-971e-2e94c3761581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What are the risks of using Generative AI?\n",
      "A: Generative AI poses significant risks such as hallucinations, bias, and deepfakes\n"
     ]
    }
   ],
   "source": [
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "questions = {\"What are the risks of using Generative AI?\"}\n",
    "for q in questions:\n",
    "    res = qa_pipeline(question=q, context=context)\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {res['answer']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfc9090-4d25-4df1-af1f-4ab03833efa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
